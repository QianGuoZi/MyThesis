\chapter{相关技术基础概述}
%
\section{协同边缘计算}
协同边缘计算是边缘计算技术面向分布式智能需求的进阶发展范式，其突破了传统云 - 边 - 端三层中心化架构的局限\cite{EaaS}。协同边缘计算通过大规模、地理分布式、异构边缘节点的自主协作与资源联动，实现计算、存储、网络与数据资源的跨节点协同调度，为自动驾驶、工业物联网、元宇宙等新兴复杂应用提供超低延迟、高可靠、泛连接的智能化服务支撑\cite{8758310}。该技术将智能能力下沉至网络边缘并实现节点间的能力聚合，核心愿景是构建边缘分布式智能生态，使边缘节点在脱离云端主导的情况下，仍能完成复杂计算任务的协同处理与服务的持续供给，是实现物联网泛在智能的核心技术方向之一。

协同边缘计算的提出与发展，本质是传统边缘计算技术的局限性与新兴应用的严苛需求之间矛盾的必然结果，其核心驱动因素可分为应用需求与技术局限两方面\cite{10818760}。新兴应用的技术需求方面，自动驾驶、元宇宙、工业物联网等应用对边缘计算提出了三大核心需求，一是超低延迟，要求毫秒级完成目标检测、轨迹规划、3D 点云重建等计算密集型任务；二是超连接与大规模部署，需支撑海量异构终端（VR 设备、工业传感器、车载终端等）的跨地理区域互联与数据交互；三是动态可靠服务，要求为高移动性终端提供跨区域的无缝服务供给，应对网络动态变化与设备移动带来的服务中断问题。其次是传统边缘计算的技术局限，传统边缘计算以云 - 边协作为核心，采用云端主导的中心化资源管理模式，存在显著短板。其一，云 - 边数据传输带来不可预测的延迟，且数据上云存在隐私泄露风险；其二，忽视边缘节点间的协作能力，单一边缘节点资源受限导致复杂任务处理能力不足；其三，中心化架构的可扩展性差，难以支撑海量终端的大规模部署与动态资源调度；其四，对云端的强依赖导致网络不稳定时服务可靠性大幅下降，无法满足高移动性应用的服务需求\cite{9411933}。

协同边缘计算通过三大核心技术特征实现分布式智能，区别于传统边缘计算的技术体系，也是其支撑新兴应用需求的关键属性，三者相互协同构成协同边缘计算的技术基础：
\begin{enumerate}[topsep = 0 pt, itemsep= 0 pt, parsep=0pt, partopsep=0pt, leftmargin=0pt, itemindent=44pt, labelsep=6pt, listparindent=24pt, label=(\arabic*)]
	\item 边缘自治：指边缘节点具备离线或本地化独立运行能力，无需与云端进行通信即可完成服务的部署、执行与调度，能够在云 - 边网络中断、延迟过高的场景下保障服务的持续供给，是实现服务可靠性的核心基础\cite{6710075}。
	\item 边缘-边缘协作：指边缘节点可与周边异构节点形成动态协作集群，实现计算资源、数据资源的跨节点共享，完成 AI 算法推理、复杂任务分布式处理等计算密集型工作；同时支持跨平台、跨架构的无缝计算与动态接入，实现边缘能力的聚合与互补\cite{varghese2017edgeasaservicedistributedcloudarchitectures}。
	\item 资源弹性：指系统针对大规模、分布式、异构的边缘环境，具备资源的自主调度与动态适配能力，可根据任务负载与节点状态完成计算、存储、网络资源的自动化分配与回收，保障边缘节点资源的高效利用与任务的最优执行\cite{9223724}。
\end{enumerate}

目前协同边缘计算的代表性体系架构为面向服务的边缘即服务（Edge-as-a-Service, EaaS）框架，该框架以服务化架构为核心，实现了边缘资源的跨节点协同管理与边缘应用的端到端开发部署，整体分为基础设施即服务（IaaS）、平台即服务（PaaS）、软件即服务（SaaS）三层，各层自上而下实现能力支撑与服务调用，构成完整的协同边缘计算技术体系，具体设计如下：
\begin{enumerate}[topsep = 0 pt, itemsep= 0 pt, parsep=0pt, partopsep=0pt, leftmargin=0pt, itemindent=44pt, labelsep=6pt, listparindent=24pt, label=(\arabic*)]
    \item 基础设施即服务（IaaS）\cite{9724454}：为协同边缘计算提供底层资源支撑，核心是解决大规模异构边缘资源的协同管理与调度问题。该层设计分布式边缘操作系统（EdgeOS）作为核心管理载体，采用容器轻量级虚拟化技术抽象异构的计算、存储、网络资源，实现服务在地理分布式边缘节点间的无缝迁移；同时设计网络调度器、计算调度器、存储调度器三大协同调度模块，联合考虑数据局部性、资源异构性与网络动态性，完成跨节点耦合资源的最优分配，满足边缘原生应用的低延迟、高吞吐量需求。
    \item 平台即服务（PaaS）\cite{9141564}：为边缘应用开发提供通用化平台能力支撑，是实现边缘分布式智能的核心层，主要提供边缘学习服务与边缘区块链服务（EBaaS）两大核心能力。其中，边缘学习服务支撑 AI 模型的全生命周期管理，涵盖数据预处理、模型开发、分布式训练（联邦学习、流言学习、E-tree 学习等范式）与资源感知推理优化（模型压缩、分区、早退出）；边缘区块链服务则将区块链组件部署于边缘节点，实现低延迟、高安全的分布式数据存证与共享，满足边缘场景下的隐私保护与数据一致性需求。两层服务均提供标准化 API，实现与底层 IaaS 层的资源调用与上层 SaaS 层的应用开发支撑。
    \item 软件即服务（SaaS）\cite{8669948}：面向具体业务场景提供边缘原生应用服务，要求应用采用模块化、无状态的设计范式，将复杂应用拆分为多个相互依赖的子模块并分布式部署于多个边缘节点，适配单一边缘节点的资源约束；同时依托底层 PaaS 与 IaaS 层提供的统一编程抽象与资源调度能力，屏蔽边缘设备的异构性与网络的动态性，实现应用的高效开发与跨节点协同运行，典型的边缘原生 SaaS 应用包括实时视频监控、智能建筑监测、自动驾驶决策等。
\end{enumerate}

协同边缘计算的落地实现依赖于多领域技术的融合支撑，结合 EaaS 框架的设计需求，其核心支撑技术围绕边缘资源管理、智能计算、数据交互、应用开发与安全保障展开，具体包括六大类\cite{EaaS}：
\begin{enumerate}[topsep = 0 pt, itemsep= 0 pt, parsep=0pt, partopsep=0pt, leftmargin=0pt, itemindent=44pt, labelsep=6pt, listparindent=24pt, label=(\arabic*)]
    \item 轻量级跨平台虚拟化技术：采用容器替代传统虚拟机，实现边缘资源的轻量级抽象，同时需适配资源受限的嵌入式边缘设备，支持 x86/ARM 等多架构、GPOS/RTOS 等多系统与 CPU/GPU/TPU 等多计算单元的跨平台部署，兼顾多租户支持与安全隔离。
    \item 边缘原生资源调度技术\cite{10631066}：针对边缘场景的耦合资源、动态网络、大规模分布式特征，设计协同调度算法，实现计算、存储、网络资源的联合调度，同时考虑设备与网络的可靠性约束，采用分布式调度模式提升大规模场景下的调度效率与可扩展性。
    \item 分布式数据共享技术：设计适配边缘场景的轻量级共识机制，解决大规模、异构、网络不稳定条件下的状态数据一致性问题，支持多应用的差异化一致性需求，实现边缘节点间的数据安全共享与同步。
    \item 资源感知边缘 AI 技术：针对边缘场景的 NonIID 数据、无标注数据、流式数据特征，设计自适应的分布式模型训练方法；同时通过模型压缩、分区、早退出等手段实现资源感知的模型推理，适配边缘节点的资源约束。
    \item 轻量化安全与隐私保护技术：开发适配边缘资源受限特征的轻量级加密、访问控制机制，防范无线通信窃听、跨节点攻击扩散等安全风险，平衡多利益相关方的数共享与隐私保护需求。
    \item 统一编程抽象技术：构建轻量级、通用化、模块化的中间件，为应用开发者提供标准化的编程接口，屏蔽边缘设备异构、网络动态、资源不确定等底层复杂性，使开发者聚焦于应用逻辑设计，实现边缘应用的端到端开发与部署。
\end{enumerate}

协同边缘计算依托低延迟、高可靠、分布式协作的技术优势，在实时视频监控、智能建筑、自动驾驶这类对计算延迟、服务可靠性与资源协同性要求严苛的新兴领域实现了典型落地并发挥显著应用价值：在实时视频监控场景，通过边-边协作满足校园、工厂、园区等中距离场景的低延迟智能分析需求，还能灵活调用云端算力支撑城市级大规模监控，兼顾效率与存储；在智能建筑场景，借助边缘节点协同通信实现各监测子系统联动，高效运行分布式数据处理算法，适配场景的低延迟与数据敏感需求；在自动驾驶场景，通过协同管理车载终端、路侧单元、基站等各类边缘资源，完成核心任务的跨节点高效卸载，解决单节点算力不足、基站网络与负载问题，保障自动驾驶决策的毫秒级执行要求。

\section{任务调度}
在边缘计算环境中，任务调度是实现系统高效运行的核心问题。该问题本质是在由云、边缘服务器和终端设备构成的异构、分布式系统中，动态地为一系列具有不同属性与需求的计算任务分配合适的计算、通信和存储资源，并确定任务的执行位置与顺序。由于该问题通常属于NP-hard复杂问题\cite{10068185,8758310}，其求解需综合考虑多类约束与多个相互冲突的目标，因此设计和评估调度算法具有重要的理论和实际意义。

边缘计算中的任务调度可形式化描述为一个在多重约束下寻求最优决策的数学优化问题，其输入、决策、目标与约束如下：

首先，问题的输入包含三个关键部分：
\begin{itemize}[topsep = 0 pt, itemsep= 0 pt, parsep=0pt, partopsep=0pt, leftmargin=0pt, itemindent=36pt, labelsep=6pt, listparindent=24pt]
    \item 待处理的任务集合：包含多个相互独立或具有依赖关系的计算任务。每个任务都有一系列特征属性，例如完成任务所需的计算强度（如CPU核数）、需要传输的数据量大小、必须完成的截止时间，以及可能存在的与其他任务的先后执行顺序（即工作流依赖关系）和多任务间需要建立的网络连接通信拓扑关系。

    \item 可用的资源集合：涵盖系统中所有可供调度的实体，包括远端云数据中心、网络边缘的服务器、以及终端设备本身。资源类型不仅指计算单元（如CPU、GPU），也包括通信资源（如网络带宽）和存储资源。

    \item 动态的系统状态：指随时间变化的系统环境信息，例如各节点之间的网络延迟与可用带宽、各个计算节点的当前负载情况、终端设备的剩余电量、以及设备的移动轨迹等。
\end{itemize}

其次，调度器需要做出的一系列决策，即输出方案，主要包括：
\begin{itemize}[topsep = 0 pt, itemsep= 0 pt, parsep=0pt, partopsep=0pt, leftmargin=0pt, itemindent=36pt, labelsep=6pt, listparindent=24pt]
    \item 为每个任务指定在何处执行（例如，本地设备、某个特定的边缘服务器或云端），这通常是一个二元选择变量。

    \item 决定为每个任务分配多少具体的资源量，例如分配多少计算能力（CPU频率）、占用多少通信带宽。

    \item 对于可以拆分的大型任务，确定有多大比例的部分被卸载到远程执行，其余部分在本地执行。

    \item 确定所有任务执行的先后顺序或开始时间。
\end{itemize}

目标函数通常为多指标的综合优化，常见方向包括：
\begin{itemize}[topsep = 0 pt, itemsep= 0 pt, parsep=0pt, partopsep=0pt, leftmargin=0pt, itemindent=36pt, labelsep=6pt, listparindent=24pt]
    \item 最小化任务完成延迟（含传输、排队、计算与回传）；
    \item 最小化系统或设备能耗；
    \item 最大化资源利用率或系统吞吐量；
    \item 在满足服务质量的前提下降低经济成本。
\end{itemize}

约束条件则包括：
\begin{itemize}[topsep = 0 pt, itemsep= 0 pt, parsep=0pt, partopsep=0pt, leftmargin=0pt, itemindent=36pt, labelsep=6pt, listparindent=24pt]
    \item 任务截止时间约束；
    \item 节点处理能力上限（CPU、内存）；
    \item 网络带宽限制；
    \item 设备能量预算；
    \item 任务间的依赖关系约束。
\end{itemize}
因此，调度问题可表述为：在满足上述约束的前提下，通过优化决策变量，使目标函数达到最优或近似最优。

边缘计算任务调度的核心目标是一个多维度且通常存在内在冲突的权衡体系，构成了算法设计与性能评估的根本依据\cite{9519636}。这些目标主要涵盖四个层面：在性能层面，首要追求最小化任务延迟（涵盖传输、排队、计算与回传的全过程时延），以满足自动驾驶、增强现实等实时应用的严苛要求，并力求最大化系统吞吐量以提升整体处理能力；在效率层面，核心关注最小化能耗以延长终端设备续航并降低运营成本，同时最大化资源利用率以实现基础设施的更经济化使用。此外，还需考量经济与服务品质，包括最小化经济成本（如资源租赁与数据传输费用）、保障服务质量与用户体验，以及通过合理机制保证多用户或多任务间的资源分配公平性，防止资源饥渴。最后，在系统稳健性层面，实现负载均衡是关键，旨在通过均匀分发任务来避免局部过载，从而提升系统的可靠性与可扩展性。实际调度算法必须综合协调这些相互竞争的目标，进而求解一个复杂的高维多目标优化问题。

针对边缘计算中复杂的任务调度问题，研究者提出了多种优化方法，各类方法在适用场景与性能上各有特点：

\begin{enumerate}[topsep = 0 pt, itemsep= 0 pt, parsep=0pt, partopsep=0pt, leftmargin=0pt, itemindent=44pt, labelsep=6pt, listparindent=24pt, label=(\arabic*)]
    \item 传统精确优化方法\cite{7879258,9475471}：为调度问题提供了形式化的数学基础。该方法通常将问题建模为混合整数线性或非线性规划模型，并借助CPLEX、Gurobi等专业求解器进行计算。其核心优势在于能够获得理论上的最优解，为性能评估提供黄金标准。然而，其计算复杂度随问题规模呈指数级增长，难以适用于大规模、高动态的真实边缘场景。因此，这类方法主要角色是进行离线理论分析、为其他启发式算法提供性能上界参照，或用于小规模静态场景的精确求解。
    \item 启发式与元启发式算法\cite{MATERWALA2022205,WANG2022109164,8811603,8981986}：为应对大规模实际问题提供了实用且高效的近似求解途径。启发式算法依赖直观规则（如最早截止时间优先）进行快速决策；而元启发式算法，如模拟生物进化过程的遗传算法、模仿鸟群协作的粒子群优化以及受热力学启发的模拟退火算法，则通过结构化地探索解空间来逼近全局优解。这类方法在可接受时间内能为大规模、非凸问题提供高质量解，且实现相对简单。但其性能往往依赖于经验性参数调优，无法保证最优性，并且在超动态环境中可能因收敛速度问题而难以适配。它们广泛应用于任务集相对固定、需在异构边缘节点间进行分配的静态或半静态调度场景。
    \item 博弈论方法\cite{XU2022107624,8931659}：从经济学与交互决策视角为调度问题提供了新颖的建模框架。它将资源提供方（边缘/云）与任务所有者（用户）建模为理性且自私的博弈参与者，通过分析纳什均衡等均衡概念，来研究分布式环境下的资源竞争、定价与分配策略。这种方法天然适合刻画边缘计算中多主体、分散决策的特性，能够有效模拟用户间的竞争与合作关系。其主要挑战在于均衡解的存在性证明与求解复杂性，且通常依赖于信息完全或部分完全的假设。该方法在多用户竞争边缘资源的卸载决策、以及服务提供商之间的资源定价策略设计等场景中具有独特价值。
    \item 基于学习的方法（如深度强化学习）\cite{9253665,8657791}：特别是机器学习与深度学习，代表了应对高度动态与复杂环境的前沿方向。其中，深度强化学习通过让智能体（即调度器）以试错方式与环境交互，并根据延迟、能耗等指标构成的奖励信号不断优化策略，从而学会自适应调度。深度学习则可用于精准预测任务需求或网络状态，甚至端到端地直接输出调度决策。这类数据驱动的方法具有强大的环境感知与在线适应能力，擅长处理高维状态空间问题。然而，它们通常需要大量的训练数据与计算资源，其决策过程如同“黑箱”般可解释性差，在安全关键型应用中需格外谨慎。该类方法在车辆边缘计算、无人机辅助移动边缘计算等网络拓扑与任务负载快速变化的场景中展现出巨大潜力。
    \item 基于李雅普诺夫优化的在线算法\cite{8638582,9239321}：提供了一种具有严格理论保证的实时决策框架。它将具有长期平均约束的优化问题（如平均功耗限制）巧妙地分解为一系列在每个时隙内可解的即时确定性子问题，并通过构造虚拟队列将约束违反转化为队列稳定性问题。这种方法的突出优点在于其在线特性——无需未来信息即可做出实时决策，并能从理论上保证系统的长期队列稳定与性能边界。其局限性在于决策本质上是贪心的，可能牺牲短期最优性，且对如何将原问题转化为李雅普诺夫框架有较高的数学技巧要求。它尤其适用于能量收集型边缘节点等资源受长期平均约束的动态调度场景。
    \item 考虑公平与负载均衡的专门方法\cite{9316512,8657791}：是一种重要的设计原则与优化视角。它强调将公平性指标（如最大最小公平、比例公平）或负载均衡指标（如节点间负载方差）明确地纳入目标函数或作为约束条件。这一理念可以与上述多种方法深度结合：例如，在博弈论中采用纳什议价解来建模公平分配；在优化模型中引入公平性权重；或在强化学习的奖励函数中设计惩罚项以促进负载均衡。这类方法对于构建健康、可持续的多租户边缘计算平台至关重要，能够确保所有用户或区域都能获得基本可用的服务，防止资源分配失衡，从而提升系统的整体稳健性与社会福祉。
\end{enumerate}

综上，边缘计算任务调度是一个多目标、多约束、动态 NP-hard 问题，其求解方法呈现多元化发展趋势，未来研究趋向于融合多种方法，以适应边缘环境的高动态、异构和安全可靠需求。

\section{分布式训练}
随着互联网技术的迅速发展，数据量急剧增加，单机处理大规模数据集变得越来越困难。这促使研究人员探索新的方法来提高数据处理能力和学习效率，分布式机器学习因此被提出。它通过在多个计算节点上并行执行算法，以加速机器学习模型的训练过程，从而实现处理大型模型、缩短训练时间和提升模型性能的目标。

在边缘计算环境中，分布式机器学习面临独特的挑战：边缘设备通常具有资源受限、异构性强、网络条件不稳定等特点，无法存储大量数据且难以实现高效的中间计算结果交换。因此，本文的研究重点是基于数据并行模式，在协同边缘计算背景下开展相关工作。

分布式机器学习按照并行模型可分为三种类型：模型并行、数据并行和混合并行\cite{Verbraeken_2020}。在模型并行中，模型的不同部分被划分到不同的计算节点进行处理，适用于模型过大而无法在单节点内存中加载的场景，但对节点间通信效率要求较高。数据并行则是将数据集分割为多个子集，分配到不同计算节点，每个节点持有完整的模型副本进行本地训练，之后通过聚合本地更新（如参数平均或梯度更新）来同步全局模型。该模式适合数据量大而模型相对较小的任务，也更适应边缘设备存储与计算受限的特点。混合并行结合了数据并行与模型并行的优势，可同时应对大规模数据与大型模型，但其设计与实现更为复杂，需综合考虑模型划分与数据划分策略。

分布式机器学习的通信步调主要分为同步通信、异步通信和半同步通信\cite{xing2015strategiesprinciplesdistributedmachine}。同步通信要求所有节点完成当前轮次后再进入下一轮计算，保证模型一致性，但可能受限于最慢节点。异步通信允许节点独立更新，提高了资源利用率，但可能因信息滞后影响收敛效果。半同步通信则尝试在两者间取得平衡，允许一定程度延迟，以减少等待时间并控制不一致性。在边缘协同环境中，网络异构与动态变化使得通信步调的选择与优化尤为关键，需要结合任务特性与资源状态进行动态调整。

当前，协同边缘计算中的分布式智能任务（如分布式训练）呈现出复杂的网络依赖拓扑，传统基于有向无环图的任务模型难以准确刻画其通信特征。同时，现有调度机制往往将计算与网络资源独立管理，忽视了二者在边缘环境中的紧密耦合与权衡关系，易引发资源竞争与性能瓶颈。因此，面向边缘环境的分布式机器学习需构建能够联合调度计算与网络资源的优化框架，以适应动态、异构的资源条件并保障任务整体效率与系统均衡性。

\section{深度强化学习}
\subsection{深度强化学习概述}
强化学习作为机器学习的一个重要分支，其核心目标在于通过与环境的交互，使智能体学会如何做出序列决策以最大化长期累积奖励。与监督学习和无监督学习不同，强化学习强调在动态和不确定的环境中通过试错进行学习，尤其适用于决策过程复杂、环境反馈延迟或稀疏的场景。

\begin{figure}[htbp]
	\centering
	\includegraphics[scale=0.6]{Fig/ces-cn_drl.png}
	\caption{\label{drl}强化学习模型}
\end{figure}

在强化学习框架中，智能体作为学习与决策的主体，通过感知环境状态并执行动作来与环境进行交互。环境则根据智能体的动作反馈新的状态和即时奖励。状态通常表示为观测的特征向量或张量，而动作可以是离散的或连续的，取决于具体任务。奖励作为环境对智能体行为的评价信号，引导智能体逐步优化其策略。策略作为状态到动作的映射，可以是确定性的，也可以是随机的，它决定了智能体在特定状态下如何行动。此外，价值函数用于评估状态或动作的长期价值，为智能体的决策提供评估依据。其中，状态价值函数 $V^\pi(s)$ 表示在状态 $s$ 下遵循策略 $\pi$ 的期望累积回报，而动作价值函数 $Q^\pi(s, a)$ 则表示在状态 $s$ 下执行动作 $a$ 后再遵循策略 $\pi$ 的期望累积回报。二者关系可通过贝尔曼方程表达：
\begin{equation}
    V^\pi(s) = \mathbb{E}_{a \sim \pi(\cdot|s), s' \sim P(\cdot|s,a)} \left[ R(s, a) + \gamma V^\pi(s') \right]
\end{equation}
以及
\begin{equation}
    Q^\pi(s, a) = \mathbb{E}_{s' \sim P(\cdot|s,a)} \left[ R(s, a) + \gamma \mathbb{E}_{a' \sim \pi(\cdot|s')} \left[ Q^\pi(s', a') \right] \right]。
\end{equation}
在部分方法中，还会构建环境模型来预测状态转移和奖励，从而辅助规划和决策。

强化学习的最终目标是学习一个最优策略 $\pi^*$，使得从初始状态开始的累积折扣奖励最大化，即：
\begin{equation}
    \pi^* = \arg\max_{\pi} \mathbb{E}_{\tau \sim p_\pi(\tau)} \left[ \sum_{t=0}^{T} \gamma^t R(s_t, a_t) \right],
\end{equation}
其中 $\tau = (s_0, a_0, s_1, a_1, \dots)$ 表示一条轨迹，$p_\pi(\tau)$ 表示在策略 $\pi$ 下轨迹的分布，$\gamma \in [0, 1]$ 是折扣因子，用于平衡即时奖励与未来奖励的重要性。

从学习方式上看，强化学习可分为在线学习与离线学习。在线学习中，智能体直接通过与环境的交互更新当前策略；而离线学习则允许智能体从历史经验回放中学习，能够利用过往策略收集的数据，提高样本效率。根据学习目标的不同，强化学习方法主要分为三类：基于价值的方法通过逼近最优价值函数来间接得到策略，其核心是学习最优动作价值函数 $Q^*(s, a)$，满足贝尔曼最优方程：
\begin{equation}
    Q^*(s, a) = \mathbb{E}_{s' \sim P} \left[ R(s, a) + \gamma \max_{a'} Q^*(s', a') \right]。
\end{equation}
经典的Q学习及其深度扩展DQN即属于此类。基于策略的方法则直接优化策略函数 $\pi_\theta(a|s)$，通过参数 $\theta$ 进行参数化，并沿期望回报的梯度方向进行更新：
\begin{equation}
    \nabla_\theta J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ \sum_{t=0}^{T} \nabla_\theta \log \pi_\theta(a_t|s_t) G_t \right],
\end{equation}
其中 $J(\theta)$ 是策略性能的度量（如期望回报），$G_t$ 是从时刻 $t$ 开始的累积回报。REINFORCE及其各种改进算法属于此类。而演员-评论员方法巧妙地将两者结合，其中“演员”负责根据策略 $\pi_\theta(a|s)$ 选择动作，“评论员”则利用价值函数 $V_\phi(s)$ 或 $Q_\phi(s, a)$ 评估动作的价值，代表性算法包括A3C、DDPG等。此外，根据是否依赖环境模型，还可以分为基于模型的方法和无模型方法。

强化学习问题通常被形式化为马尔可夫决策过程。MDP假设系统具有马尔可夫性，即未来状态仅依赖于当前状态和动作，而与历史状态无关，由五元组 $\langle \mathcal{S}, \mathcal{A}, P, R, \gamma \rangle$ 定义。

深度强化学习将深度学习的强大表征能力与强化学习的序列决策能力相结合，通过深度神经网络来近似价值函数、策略或环境模型。这一融合使得智能体能够处理高维、复杂的原始输入数据，从而在诸多领域取得了里程碑式的成就。然而，DRL也面临着若干挑战，例如样本效率低下、探索与利用之间的平衡难题、训练过程的不稳定性，以及稀疏奖励环境下的学习困难等。

\subsection{近端策略优化算法原理}
\label{subsec:ppo-algorithm}

近端策略优化算法Proximal Policy Optimization(PPO)\cite{PPO2017}是一种基于策略梯度的深度强化学习方法，其核心设计思想在于通过引入一种特殊的优化目标来约束策略更新的幅度，从而在追求性能提升的同时，确保训练过程的稳定性。

PPO建立在策略梯度方法的基础之上。PPO算法通过优化一个精心设计的替代目标函数来实现策略的渐进式改进。该目标函数的核心是“裁剪”机制。具体而言，算法首先计算新旧策略的概率比值：
\begin{equation}
    r_t(\theta) = \frac{\pi_\theta(a_t | s_t)}{\pi_{\theta_{\text{old}}}(a_t | s_t)},
\end{equation}
其中 $\pi_{\theta_{\text{old}}}$ 是更新前的旧策略参数。PPO的目标是最大化一个裁剪后的优势函数期望值。其裁剪目标函数定义为：
\begin{equation}
    L^{\text{CLIP}}(\theta) = \hat{\mathbb{E}}_t \left[ \min\left( r_t(\theta) \hat{A}_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon) \hat{A}_t \right) \right],
\end{equation}
其中 $\hat{A}_t$ 是时刻 $t$ 的优势函数估计值，衡量了特定动作相对于平均水平的优劣；$\epsilon$ 是一个小超参数（通常为0.1或0.2），用于定义裁剪区间。$\text{clip}(x, l, u)$ 操作将 $x$ 限制在 $[l, u]$ 之间。该公式通过取最小值，确保了目标函数是原始比值项与裁剪后比值项的下界，从而避免了因策略更新过大（$r_t(\theta)$ 偏离1过远）而导致的性能崩溃。

优势函数 $\hat{A}_t$ 通常通过广义优势估计进行计算：
\begin{equation}
    \hat{A}_t = \delta_t + (\gamma \lambda) \delta_{t+1} + \dots + (\gamma \lambda)^{T-t+1} \delta_{T-1},
\end{equation}
其中 $\delta_t = r_t + \gamma V(s_{t+1}) - V(s_t)$ 是时序差分误差，$\lambda$ 是权衡偏差与方差的参数。

为了提升性能与稳定性，PPO的完整目标函数通常结合了价值函数损失和策略熵奖励：
\begin{equation}
    L^{\text{PPO}}(\theta, \phi) = \hat{\mathbb{E}}_t \left[ L^{\text{CLIP}}_t(\theta) - c_1 L^{\text{VF}}_t(\phi) + c_2 S[\pi_\theta](s_t) \right],
\end{equation}
其中 $L^{\text{VF}}_t(\phi) = (V_\phi(s_t) - V_t^{\text{targ}})^2$ 是价值函数 $V_\phi$ 的均方误差损失，$S[\pi_\theta](s_t)$ 是策略在状态 $s_t$ 下的熵，用于鼓励探索，$c_1$ 和 $c_2$ 是系数。

相较于早期的策略优化方法，如信赖域策略优化，PPO在保持相近性能的同时，实现上更为简洁高效。其训练流程通常包括使用当前策略交互采样、计算优势估计、以及进行多轮小批量梯度更新等步骤。由于其在连续和离散动作空间中的良好表现、优异的稳定性以及较高的样本效率，PPO已成为最受欢迎的深度强化学习算法之一，被广泛应用于机器人控制、游戏智能、自动驾驶等诸多领域\cite{10703056}。

\section{本章小结}
本章系统阐述了协同边缘计算、任务调度、分布式训练与深度强化学习四个基础技术领域，为后续研究提供理论支撑。首先，从协同边缘计算的演进、核心特征、体系架构及支撑技术入手，阐明边缘分布式智能平台的关键作用。其次，分析任务调度问题的形式化描述、优化目标及典型求解方法。随后，聚焦分布式训练在边缘场景中的应用挑战。最后，介绍深度强化学习基本原理，并重点剖析近端策略优化算法，为自适应调度算法设计提供方法论基础。通过对上述技术的系统梳理与分析，本章为后续章节中边缘任务调度系统与算法设计与评估奠定了坚实的理论基础，并明确了研究的核心问题与技术路径。