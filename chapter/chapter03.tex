\chapter{协同边缘计算任务调度系统CES设计}
本节将详细阐述CES协同边缘计算调度系统的设计思路、核心架构与关键流程。首先，本节将明确系统旨在解决的核心问题与设计目标；其次，将深入剖析基于主从架构的系统组件设计与交互机制；最后，将系统地描述一个作业从提交到执行完毕的任务调度过程。

\section{系统设计目标}
CES协同边缘计算调度系统是一种面向智能边缘环境的任务调度系统，旨在解决弹性环境中具有通信依赖关系的分布式任务调度问题。此处定义的“弹性环境”包含双重动态性：一是计算与网络物理资源可随节点加入或离开而动态变化；二是待处理的任务规模与资源需求可随时间动态波动。在此复杂环境下，传统的、孤立考虑计算或网络资源的调度策略难以实现整体效能最优。

因此，本系统的核心设计目标是实现对计算资源与网络资源的统一感知与联合调度。系统需构建一个全局的、融合的资源视图，统一纳管任务数据布局、节点计算能力与网络链路状态，并以此为基础进行协同决策。具体而言，CES系统旨在达成以下三个关键设计指标：
\begin{itemize}
\item \textbf{确保系统在真实边缘环境下的调度可信度}：系统需直接部署并运行于真实的边缘计算环境中，该环境天然具备节点的异构性（如计算能力与网络接口的差异）、资源的动态弹性以及网络拓扑的复杂性。调度系统必须能够在此类真实、动态的条件下，持续感知环境变化，并生成与执行有效的调度决策。其目标是保证系统在实际部署中的调度行为与结果真实、可靠，从而验证其工程实用性与有效性。

\item \textbf{提升跨边缘节点的资源利用均衡度}：避免部分节点过载而其他节点空闲的资源碎片化现象。调度算法应综合考虑CPU、内存及网络等多维资源，力求在系统全局范围内实现负载均衡，从而提高基础设施的整体资源利用率与投资回报率。

\item \textbf{保障作业间的性能隔离}：在共享的、多租户边缘环境中，确保不同作业的运行环境相互隔离，避免资源争用导致的性能干扰。系统需通过容器化技术与细粒度的资源配额管理，强制实施CPU、内存及网络带宽的资源隔离，为共存的作业提供独立、可控的执行环境，保障其性能的独立性与可预期性。
\end{itemize}

为实现上述目标，CES系统被设计为一个覆盖“资源感知-智能决策-精准执行”的任务调度系统。该系统不仅强调调度算法在理论上的优化能力，更注重其在实际异构、动态边缘环境中的工程可实现性与鲁棒性，从而最终为用户提供一个稳定、高效且资源感知的边缘计算服务基底。

\begin{figure}[htbp]
	\centering
	\includegraphics[scale=0.45]{Fig/ces-cn_system_framework.png}
    \caption{\label{system_framework}协同边缘任务调度系统CES架构图}
\end{figure}

\section{系统架构}
为高效协同异构、分布式的边缘计算资源，并满足分布式训练任务对计算与网络资源的联合需求，本系统采用经典的Client-Server架构进行设计。该架构通过中心化的协调与分布式的执行相结合，实现了资源管理的统一性与任务执行的可扩展性。系统主要由一个中心化的主节点（Master） 和多个分布式的边缘节点（Edge Node） 构成。其中，Master节点作为系统的大脑，负责全局资源的管理、作业解析与全局任务调度；而Edge Node作为系统的四肢，负责接收调度指令，并通过容器化技术提供隔离、可控的执行环境来具体完成任务。整体架构的核心目标是在动态、异构的边缘环境中，实现计算与网络资源的协同分配与高效调度。CES系统的具体架构如图\ref{system_framework}所示。


\subsection{Master节点设计}
Master节点是系统的控制中心，其设计着重于全局状态管理、决策制定以及与客户端的交互。它包含以下关键组件，各组件通过内部接口进行高效协作：
\begin{itemize}
\item \textbf{管理器（Manager）}：该组件作为系统对外的首要接口，承担作业的接入与解析职责。它接收用户提交的分布式训练作业描述，解析其中的任务、通信拓扑及资源需求。解析完成后，Manager将结构化的作业信息传递给调度器（Scheduler），并负责将作业状态、执行结果等反馈给用户，管理作业的生命周期。
\item \textbf{控制器（Controller）}：Controller是资源管理的执行层，包含两个专用于处理不同资源维度的子控制器：
\begin{itemize}
    \item \textbf{计算资源控制器（Compute Controller）}：该控制器基于Docker容器技术实现对底层异构计算资源的统一虚拟化抽象与管理。它负责根据调度器的指令，在指定的边缘节点上创建、启动、暂停或销毁Worker容器，从而为每个任务提供计算隔离的运行环境。同时，Compute Controller持续从各边缘节点收集细粒度的资源监控数据（包括CPU利用率、内存占用等），不仅为Scheduler的决策提供实时、准确的依据，也确保了资源分配的公平性和隔离性。
    \item \textbf{网络资源控制器（Network Controller）}：为满足分布式训练中频繁的参数同步与数据交换对网络性能的严格要求，该控制器负责管理边缘节点间的网络资源。其功能涵盖带宽的分配、数据流的路由策略制定以及跨节点的流量转发控制。通过与计算资源的协同管理，Network Controller旨在减少通信瓶颈，保障作业的网络服务质量（QoS）。
\end{itemize}
\item \textbf{调度器（Scheduler）}：作为Master节点的核心决策组件，Scheduler负责协调Compute Controller与Network Controller，实施跨资源维度的联合调度。它持续收集来自各控制器的系统全局信息，包括但不限于所有边缘设备的实时资源利用率、网络链路状态与拓扑、以及排队等待作业的详细配置。基于这些信息，Scheduler运行内嵌的任务调度算法，生成最优的作业调度策略与资源分配方案。其调度决策直接决定了任务被派往哪个边缘节点、获得多少资源以及使用怎样的网络路径，最终目标是优化系统整体的负载均衡度等关键性能指标。
\end{itemize}

\subsection{Edge Node节点设计}
Edge Node是部署在边缘侧的物理或虚拟设备，负责提供具体的计算和网络能力。每个Edge Node在设计上需具备自治的资源管理能力和与Master协同工作的通信能力，其内部组件如下：
\begin{itemize}
\item \textbf{通信器（Messenger）}：该组件是Edge Node与外界通信的枢纽。它一方面与Master节点保持持久的心跳连接，定期上报本节点的资源状态（通过Manager收集），并接收来自Master的调度指令与控制命令（如创建Worker）；另一方面，在需要节点间直接通信的任务中（如分布式训练中的参数服务器与Worker之间），Messenger也负责与其他Edge Node建立点对点的数据通信通道，高效传输中间数据或模型参数。
\item \textbf{管理器（Manager）}：每个Edge Node拥有一个本地的管理器，它是Master端Controller在边缘侧的代理和执行延伸，也包含两个部分：
\begin{itemize}
    \item \textbf{计算资源管理器（Compute Manager）}：负责管理本节点所有计算资源的具体分配。它接收并执行来自Master Compute Controller的容器操作指令，通过调用本地的Docker来实际创建和管理Worker容器。同时，它持续监控本节点各容器的资源消耗情况，并将聚合后的数据通过Messenger上报。
    \item \textbf{网络资源管理器（Network Manager）}：负责执行Master的Network Controller下发的网络策略。它在本地通过流量控制工具Traffic Control来实施具体的带宽限制、流量整形或路由规则，确保节点出/入口的网络流量符合全局调度方案的要求。
\end{itemize}
\item \textbf{工作器（Worker）}：Worker是由Docker容器实例化的任务执行单元，是实际执行用户作业中具体任务的实体。每个Worker运行在资源隔离的容器环境中，内部包含用户指定的训练框架、应用程序代码及依赖库。Worker根据作业逻辑进行本地计算，并通过Messenger组件与其他Worker或参数服务器进行通信，共同完成分布式训练任务。
\end{itemize}

综上所述，本系统通过Master节点的集中式智能调度与Edge Node节点的分布式高效执行相结合，构建了一个层次化、松耦合的协同计算框架。各组件各司其职又紧密联动，为在复杂边缘环境下开展资源敏感的分布式训练任务提供了坚实的系统基础。

\section{任务调度流程}
该系统的任务调度流程遵循从作业描述到资源分配、最终至任务执行的清晰逻辑过程，旨在实现跨资源维度的协同优化。整个过程可划分为三个阶段，其详细流程如下：

首先，在作业提交与解析阶段，用户通过客户端向Master节点的Manager组件提交分布式训练作业。该作业通常以资源描述文件进行定义，其中完整定义了作业的规格，包括：任务类型、待执行的程序实体（如镜像名称或脚本路径）、计算资源需求（例如，所需的CPU核数及内存大小）、任务间的网络带宽需求及通信拓扑。Manager接收并解析此作业描述，将其转化为系统内部可识别的结构化任务元数据，为后续的智能调度提供精确的决策依据。

随后，进入资源联合调度与决策阶段，此为系统的核心环节。Master中的Scheduler（即联合调度器）被触发，它综合多源信息进行全局决策：
\begin{itemize}
\item \textbf{任务侧信息}：来自Manager的作业元数据，特别是任务资源需求与通信拓扑。
\item \textbf{系统侧信息}：Controller持续收集并维护的全局资源视图，包括各Edge Node的实时计算资源利用率、可用网络带宽以及链路拓扑。
\end{itemize}

基于上述信息，Scheduler运行其内嵌的协同调度算法。该算法旨在平衡边缘节点的负载、最小化通信开销并满足资源约束，最终生成一个细粒度的调度策略。此策略具体规定了：1) 每个任务实例被分配到的目标Edge Node；2) 为任务间关键数据流分配的带宽配额；3) 高效的数据传输路径或路由方案。此策略体现了系统“数据与计算协同感知”的核心设计思想。

最后，在任务分发、执行与监控阶段，调度策略被下发并执行。Master的Controller将任务启动指令及资源配置要求下发至目标Edge Node。各Edge Node的本地Manager协同工作：Compute Manager根据指令通过Docker创建指定规格的Worker容器；Network Manager则配置本地的流量控制规则以实施分配的带宽方案。任务开始在隔离的容器中执行，期间所有Worker的状态（如运行、完成、失败）及各节点的资源消耗数据通过Messenger组件实时反馈至Master的Controller。这些动态信息构成了一个闭环反馈，使得Scheduler能够进行潜在的动态任务迁移或资源再分配，以应对负载波动或节点故障，从而保障系统的整体鲁棒性与执行效率。

综上所述，该工作流程通过“解析-决策-执行-反馈”的过程，实现了对边缘异构资源的精细化管理与自适应调度，确保了分布式训练作业在复杂边缘环境中的高效、可靠运行。

\section{本章小结}
本章详细阐述了协同边缘计算任务调度系统CES的设计与实现。首先明确了系统面向弹性环境中具有通信依赖的分布式训练任务，确立了计算与网络资源联合调度的核心设计目标，具体包括提升真实环境调度可信度、资源利用均衡度与作业性能隔离。随后，系统采用主从架构，详细设计了中心化Master节点与分布式Edge Node节点的内部结构及交互机制，实现了全局资源统一视图与协同决策。最后，系统描述了从作业提交、资源联合调度到任务分发执行的完整调度流程，通过“解析-决策-执行-反馈”闭环机制保障了动态环境下的调度鲁棒性。本章为后续算法设计与实验验证提供了完整的系统支撑。